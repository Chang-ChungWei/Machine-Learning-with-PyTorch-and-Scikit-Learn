{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning with PyTorch and Scikit-Learn  \n",
    "# -- Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package version checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add folder to path in order to load from the check_packages.py script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解了！你想在你的Python環境中導入上級目錄（即父目錄）的模組或程式庫。這在處理專案結構或需要訪問其他目錄中的模組時很有用。\n",
    "\n",
    "通過將上級目錄插入到`sys.path`中，可以讓Python解釋器找到並載入該目錄中的模組。這在開發和測試時尤其有用，可以避免將所有模組都放在同一目錄下，從而更好地組織代碼。\n",
    "\n",
    "如果你有任何具體的問題或需要進一步的說明，請隨時告訴我！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check recommended package versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Your Python version is 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) \n",
      "[GCC 9.3.0]\n",
      "[OK] pandas 1.3.5\n",
      "[OK] torch 1.10.0\n",
      "[OK] torchtext 0.11.0\n",
      "[OK] datasets 1.11.0\n",
      "[OK] transformers 4.9.1\n"
     ]
    }
   ],
   "source": [
    "from python_environment_check import check_packages\n",
    "\n",
    "\n",
    "d = {\n",
    "    'pandas': '1.3.2',\n",
    "    'torch': '1.9.0',\n",
    "    'torchtext': '0.11.0',\n",
    "    'datasets': '1.11.0',\n",
    "    'transformers': '4.9.1',\n",
    "}\n",
    "check_packages(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你想確認這些套件是否安裝在你的Python環境中，可以透過以下步驟來檢查：\n",
    "\n",
    "1. **使用pip查看套件版本**：在終端或命令提示字元中執行以下命令，可以確認套件是否安裝以及其版本：\n",
    "\n",
    "   ```bash\n",
    "   pip show pandas torch torchtext datasets transformers\n",
    "   ```\n",
    "\n",
    "   這將顯示每個套件的詳細資訊，包括版本號碼。\n",
    "\n",
    "2. **在Python中執行**：你也可以在Python中執行以下程式碼來確認這些套件的版本：\n",
    "\n",
    "   ```python\n",
    "   import pandas\n",
    "   import torch\n",
    "   import torchtext\n",
    "   import datasets\n",
    "   import transformers\n",
    "\n",
    "   print(\"pandas 版本:\", pandas.__version__)\n",
    "   print(\"torch 版本:\", torch.__version__)\n",
    "   print(\"torchtext 版本:\", torchtext.__version__)\n",
    "   print(\"datasets 版本:\", datasets.__version__)\n",
    "   print(\"transformers 版本:\", transformers.__version__)\n",
    "   ```\n",
    "\n",
    "   這將列印出每個套件的版本資訊。\n",
    "\n",
    "如果你需要進一步的幫助或有其他問題，請隨時告訴我！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Transformers – Improving Natural Language Processing with Attention Mechanisms (Part 3/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "- [Fine-tuning a BERT model in PyTorch](#Fine-tuning-a-BERT-model-in-PyTorch)\n",
    "  - [Loading the IMDb movie review dataset](#Loading-the-IMDb-movie-review-dataset)\n",
    "  - [Tokenizing the dataset](#Tokenizing-the-dataset)\n",
    "  - [Loading and fine-tuning a pre-trained BERT model](#[Loading-and-fine-tuning-a-pre-trained-BERT-model)\n",
    "  - [Fine-tuning a transformer more conveniently using the Trainer API](#Fine-tuning-a-transformer-more-conveniently-using-the-Trainer-API)\n",
    "- [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Quote from https://huggingface.co/transformers/custom_datasets.html:\n",
    "\n",
    "> DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased , runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這行程式碼是在從IPython.display模組中導入Image類別。這個類別用於在IPython環境中顯示圖像。通常，你可以使用這個Image類別來載入並顯示本地檔案系統中的圖片，或者顯示從網路上獲取的圖片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a BERT model in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vY4SK0xKAJgm"
   },
   "source": [
    "### Loading the IMDb movie review dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讓我逐行解釋這些import語句的含意：\n",
    "\n",
    "1. `import gzip`: 導入Python的gzip模組，用於處理gzip壓縮文件。\n",
    "   \n",
    "2. `import shutil`: 導入Python的shutil模組，用於高級文件操作，例如拷貝、刪除、移動文件和目錄。\n",
    "\n",
    "3. `import time`: 導入Python的time模組，提供時間操作的函數，例如sleep()。\n",
    "\n",
    "4. `import pandas as pd`: 導入Pandas庫，並使用`pd`作為別名。Pandas是用於數據處理和分析的Python庫，提供了強大的數據結構和工具。\n",
    "\n",
    "5. `import requests`: 導入Python的requests庫，用於向網絡服務器發送HTTP請求和處理響應。\n",
    "\n",
    "6. `import torch`: 導入PyTorch庫，用於深度學習和科學計算。\n",
    "\n",
    "7. `import torch.nn.functional as F`: 導入PyTorch的nn.functional模組並使用F作為別名。這個模組包含了各種用於神經網絡操作的函數，如激活函數、損失函數等。\n",
    "\n",
    "8. `import torchtext`: 導入PyTorch的torchtext庫，用於自然語言處理中文本數據的加載、處理和預處理。\n",
    "\n",
    "9. `import transformers`: 導入transformers庫，這是Hugging Face開發的一個庫，用於自然語言處理中的預訓練模型、tokenization和fine-tuning等任務。\n",
    "\n",
    "10. `from transformers import DistilBertTokenizerFast`: 從transformers庫中導入DistilBertTokenizerFast類別。這是一個快速的DistilBERT模型的分詞器，用於將文本轉換為模型可以處理的格式。\n",
    "\n",
    "11. `from transformers import DistilBertForSequenceClassification`: 從transformers庫中導入DistilBertForSequenceClassification類別。這是一個DistilBERT模型的分類器，用於處理文本序列分類任務，如情感分析或文本分類。\n",
    "\n",
    "這些import語句構建了一個Python環境，使得你可以在項目中使用各種庫和工具來進行數據處理、深度學習建模以及自然語言處理任務。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSRL42Qgy8I8"
   },
   "source": [
    "**General Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvW1RgfepCBq"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段代碼設置了一些用於深度學習模型訓練的基本配置。讓我逐行解釋：\n",
    "\n",
    "1. `torch.backends.cudnn.deterministic = True`: 這行代碼設置了CuDNN的deterministic模式為True。CuDNN是NVIDIA針對深度神經網絡的庫，deterministic模式確保在相同的輸入下，每次運行的結果都是一致的，這對於結果的可重現性很重要。\n",
    "\n",
    "2. `RANDOM_SEED = 123`: 設置了一個隨機種子為123。這個隨機種子將用於各種隨機初始化，如PyTorch中的隨機初始化、數據加載時的隨機分割等，確保每次運行的結果都是一致的。\n",
    "\n",
    "3. `torch.manual_seed(RANDOM_SEED)`: 使用上述隨機種子設置PyTorch的隨機種子，確保在可重現的情況下使用同一組隨機數。\n",
    "\n",
    "4. `DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`: 這行代碼根據系統中是否有CUDA可用來選擇設備。如果CUDA可用，設備將被設置為CUDA（GPU），否則設置為CPU。\n",
    "\n",
    "5. `NUM_EPOCHS = 3`: 設置了訓練迭代的次數，即訓練過程中數據將被遍歷的次數。\n",
    "\n",
    "總結來說，這段代碼確保了在訓練深度學習模型時，使用固定的隨機種子以及特定的設備（GPU或CPU），從而確保訓練過程的結果是可預測和可重現的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQMmKUEisW4W"
   },
   "source": [
    "**Download Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells will download the IMDB movie review dataset (http://ai.stanford.edu/~amaas/data/sentiment/) for positive-negative sentiment classification in as CSV-formatted file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/rasbt/machine-learning-book/raw/main/ch08/movie_data.csv.gz\"\n",
    "filename = url.split(\"/\")[-1]\n",
    "\n",
    "with open(filename, \"wb\") as f:\n",
    "    r = requests.get(url)\n",
    "    f.write(r.content)\n",
    "\n",
    "with gzip.open('movie_data.csv.gz', 'rb') as f_in:\n",
    "    with open('movie_data.csv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段程式碼從指定的URL下載壓縮的CSV文件，並將其解壓縮為本地的CSV文件。以下是每行程式碼的詳細解釋：\n",
    "\n",
    "1. **定義URL和文件名稱：**\n",
    "   ```python\n",
    "   url = \"https://github.com/rasbt/machine-learning-book/raw/main/ch08/movie_data.csv.gz\"\n",
    "   filename = url.split(\"/\")[-1]\n",
    "   ```\n",
    "   - `url`: 指定要下載的文件的URL。\n",
    "   - `filename`: 從URL中提取的文件名稱。在這個例子中，它會是`movie_data.csv.gz`。\n",
    "\n",
    "2. **下載壓縮文件：**\n",
    "   ```python\n",
    "   with open(filename, \"wb\") as f:\n",
    "       r = requests.get(url)\n",
    "       f.write(r.content)\n",
    "   ```\n",
    "   - 使用`requests`庫向指定的URL發送GET請求(`requests.get(url)`)，並將返回的內容寫入本地文件(`f.write(r.content)`)。\n",
    "   - `with open(filename, \"wb\") as f:` 打開二進制寫入模式的文件，如果文件不存在，將會被創建。\n",
    "\n",
    "3. **解壓縮壓縮文件：**\n",
    "   ```python\n",
    "   with gzip.open('movie_data.csv.gz', 'rb') as f_in:\n",
    "       with open('movie_data.csv', 'wb') as f_out:\n",
    "           shutil.copyfileobj(f_in, f_out)\n",
    "   ```\n",
    "   - `gzip.open('movie_data.csv.gz', 'rb')`: 打開壓縮的`movie_data.csv.gz`文件為讀取二進制模式。\n",
    "   - `open('movie_data.csv', 'wb')`: 打開要寫入解壓縮數據的`movie_data.csv`文件為二進制寫入模式。\n",
    "   - `shutil.copyfileobj(f_in, f_out)`: 將從`f_in`讀取的數據直接寫入`f_out`，從而將壓縮文件解壓縮為本地的CSV文件。\n",
    "\n",
    "這樣，你可以從指定的URL下載壓縮的CSV文件並將其解壓縮為本地的可用文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the dataset looks okay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以在本地機器上執行這段程式碼，它會將名為`movie_data.csv`的CSV文件讀取到Pandas DataFrame中，然後顯示前幾行數據。這樣你可以查看CSV文件的內容和結構。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你已經成功讀取了`movie_data.csv`文件到Pandas DataFrame中，你可以輕鬆地使用`.shape`屬性來查看DataFrame的形狀（即行數和列數）。\n",
    "\n",
    "通常情況下，這個屬性會返回一個元組，第一個元素代表行數，第二個元素代表列數。例如，如果你的DataFrame名稱是`df`，那麼可以這樣查看形狀：\n",
    "\n",
    "```python\n",
    "df.shape\n",
    "```\n",
    "\n",
    "這樣就能顯示出DataFrame的行數和列數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Dataset into Train/Validation/Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df.iloc[:35000]['review'].values\n",
    "train_labels = df.iloc[:35000]['sentiment'].values\n",
    "\n",
    "valid_texts = df.iloc[35000:40000]['review'].values\n",
    "valid_labels = df.iloc[35000:40000]['sentiment'].values\n",
    "\n",
    "test_texts = df.iloc[40000:]['review'].values\n",
    "test_labels = df.iloc[40000:]['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段程式碼從已讀入的CSV檔案中提取了訓練、驗證和測試集的文本資料和情感標籤。\n",
    "\n",
    "1. **訓練集提取**：\n",
    "   ```python\n",
    "   train_texts = df.iloc[:35000]['review'].values\n",
    "   train_labels = df.iloc[:35000]['sentiment'].values\n",
    "   ```\n",
    "   - `train_texts`：從DataFrame `df` 中提取了前35000行的 `review` 列，即電影評論文本的數據，並轉換為NumPy數組。\n",
    "   - `train_labels`：從DataFrame `df` 中提取了前35000行的 `sentiment` 列，即相應的情感標籤（正面或負面），同樣轉換為NumPy數組。\n",
    "\n",
    "2. **驗證集提取**：\n",
    "   ```python\n",
    "   valid_texts = df.iloc[35000:40000]['review'].values\n",
    "   valid_labels = df.iloc[35000:40000]['sentiment'].values\n",
    "   ```\n",
    "   - `valid_texts`：從DataFrame `df` 中提取了第35000行到第39999行的 `review` 列，作為驗證集的電影評論文本，轉換為NumPy數組。\n",
    "   - `valid_labels`：從DataFrame `df` 中提取了第35000行到第39999行的 `sentiment` 列，作為驗證集的情感標籤，轉換為NumPy數組。\n",
    "\n",
    "3. **測試集提取**：\n",
    "   ```python\n",
    "   test_texts = df.iloc[40000:]['review'].values\n",
    "   test_labels = df.iloc[40000:]['sentiment'].values\n",
    "   ```\n",
    "   - `test_texts`：從DataFrame `df` 中提取了從第40000行開始至最後一行的 `review` 列，作為測試集的電影評論文本，轉換為NumPy數組。\n",
    "   - `test_labels`：從DataFrame `df` 中提取了從第40000行開始至最後一行的 `sentiment` 列，作為測試集的情感標籤，轉換為NumPy數組。\n",
    "\n",
    "這樣的分割方式可以有效地將數據集劃分為訓練、驗證和測試三部分，以便後續進行機器學習模型的訓練、調參和評估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這行代碼是用來從Hugging Face的Transformers庫中加載預訓練的DistilBERT tokenizer。讓我來詳細解釋每一部分的意義：\n",
    "\n",
    "1. **`from_pretrained('distilbert-base-uncased')`**：\n",
    "   - `from_pretrained` 是一個Transformer庫中的通用方法，用於從預訓練模型中加載模型或tokenizer。\n",
    "   - `'distilbert-base-uncased'` 是一個預訓練的DistilBERT模型的名稱。這裡的`'uncased'` 表示模型是通過將所有文本轉換成小寫來訓練的，而 `'base'` 表示這是DistilBERT的基本版本，不含任何添加的層或特殊設置。\n",
    "\n",
    "2. **`DistilBertTokenizerFast`**：\n",
    "   - `DistilBertTokenizerFast` 是DistilBERT的一個特殊類，用於快速地進行tokenize和token轉換。這個類能夠處理長文本，並使用多線程進行更快的處理。\n",
    "\n",
    "3. **`tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')`**：\n",
    "   - 這行代碼將從Hugging Face模型庫中加載指定名稱的DistilBERT tokenizer。一旦加載完成，`tokenizer` 就會成為一個可以用於tokenize文本的實例。\n",
    "\n",
    "通常，使用這樣的tokenizer對文本進行tokenize是為了準備輸入資料以供DistilBERT或其他基於Transformer的模型進行處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段程式碼使用了之前加載的`DistilBertTokenizerFast`將訓練集、驗證集和測試集的文本進行了tokenize和編碼。讓我來解釋每行的意義：\n",
    "\n",
    "1. **`train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)`**：\n",
    "   - `tokenizer` 是之前從`DistilBertTokenizerFast.from_pretrained`加載的DistilBERT tokenizer的實例。\n",
    "   - `list(train_texts)` 是訓練集中的所有文本，以列表形式提供給tokenizer。\n",
    "   - `truncation=True` 表示如果文本長度超過DistilBERT模型的最大輸入長度，將會對文本進行截斷。\n",
    "   - `padding=True` 表示tokenizer會將所有文本進行填充，確保它們具有相同的長度，以便於批次處理。\n",
    "\n",
    "2. **`valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)`** 和 **`test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)`**：\n",
    "   - 這兩行代碼與第一行相似，只是分別將驗證集和測試集的文本列表提供給tokenizer進行tokenize和編碼。\n",
    "\n",
    "在這些編碼之後，`train_encodings`、`valid_encodings`和`test_encodings`會包含tokenized後的文本，並且已經進行了填充和截斷以準備輸入到DistilBERT或其他基於Transformer的模型中進行訓練或預測。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_encodings[0]`返回了一個`Encoding`對象，這是由Hugging Face的`tokenizers`庫生成的。這個對象包含了一些重要的屬性，這些屬性描述了經過tokenize和編碼後的文本信息。讓我來詳細解釋一下這些屬性的意義：\n",
    "\n",
    "- **`ids`**: 這是tokenize後的token IDs列表，即每個token被映射到其對應的ID。\n",
    "\n",
    "- **`type_ids`**: 在BERT和其變種模型中，這個屬性通常用於區分不同句子的token。在一般的情況下，它會是一個全為0的列表，因為大多數任務都只有一個句子。對於DistilBERT這種單一句子任務，這個屬性的作用不大。\n",
    "\n",
    "- **`tokens`**: 這是tokenize後的token字符串列表，即原始文本被分成的token。\n",
    "\n",
    "- **`offsets`**: 這個屬性對應於每個token在原始文本中的起始和結束位置的元組列表。這在需要將模型的輸出映射回原始文本位置時非常有用。\n",
    "\n",
    "- **`attention_mask`**: 這是一個二進制的遮罩向量，顯示哪些位置是真實的token（1），哪些位置是填充的token（0）。\n",
    "\n",
    "- **`special_tokens_mask`**: 如果tokenizer使用了特殊token（如`[CLS]`, `[SEP]`），則這個屬性會顯示這些特殊token的位置。\n",
    "\n",
    "- **`overflowing`**: 如果文本超出了tokenizer的最大token長度（例如BERT的512個token），那麼這個屬性將包含超出部分的附加token。\n",
    "\n",
    "總結來說，這些屬性組合起來描述了經過DistilBERT tokenizer處理後的文本，使得它們可以被模型處理和理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Class and Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "valid_dataset = IMDbDataset(valid_encodings, valid_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段代碼定義了一個`IMDbDataset`類別，這個類別繼承自PyTorch的`torch.utils.data.Dataset`，用於創建IMDb數據集的數據加載器。讓我來詳細解釋每一行的意義：\n",
    "\n",
    "1. **`class IMDbDataset(torch.utils.data.Dataset):`**: 定義了一個新的類別`IMDbDataset`，它繼承自`torch.utils.data.Dataset`，這是PyTorch中用於自定義數據集的基類。\n",
    "\n",
    "2. **`def __init__(self, encodings, labels):`**: 這是類別的初始化方法。它接受兩個參數：\n",
    "   - `encodings`: 包含了tokenize和編碼後的文本數據，通常是從tokenizer返回的`train_encodings`、`valid_encodings`或`test_encodings`。\n",
    "   - `labels`: 包含了每個文本對應的標籤數據，例如正面情感或負面情感，通常是`train_labels`、`valid_labels`或`test_labels`。\n",
    "\n",
    "3. **`self.encodings = encodings`**: 將`encodings`參數保存為類別屬性`encodings`，以便在類別的其他方法中使用。\n",
    "\n",
    "4. **`self.labels = labels`**: 將`labels`參數保存為類別屬性`labels`，以便在類別的其他方法中使用。\n",
    "\n",
    "5. **`def __getitem__(self, idx):`**: 這個方法是Dataset類別必須實現的方法之一，用於根據給定的索引`idx`返回數據樣本。在這個類別中：\n",
    "   - 創建一個名為`item`的字典，用於存儲從`encodings`和`labels`中取出的數據。\n",
    "   - 使用列表推導式，遍歷`encodings.items()`，並將每個鍵值對的值（即token IDs、attention mask等）轉換為PyTorch的tensor。\n",
    "   - 將`labels`也轉換為PyTorch的tensor，並放入`item`字典中作為`'labels'`鍵的值。\n",
    "   - 返回這個`item`字典作為索引`idx`對應的數據樣本。\n",
    "\n",
    "6. **`def __len__(self):`**: 這個方法同樣是Dataset類別必須實現的方法之一，返回數據集的長度（即樣本數量）。在這個類別中，它返回`labels`的長度，這樣在訓練或測試時，PyTorch的`DataLoader`就知道要迭代多少次了。\n",
    "\n",
    "7. **`train_dataset = IMDbDataset(train_encodings, train_labels)`**: 通過使用`IMDbDataset`類別來創建`train_dataset`，將訓練集的tokenize和編碼數據`train_encodings`與對應的標籤`train_labels`進行結合。\n",
    "\n",
    "8. **`valid_dataset = IMDbDataset(valid_encodings, valid_labels)`**: 同樣地，創建`valid_dataset`來存儲驗證集的數據。\n",
    "\n",
    "9. **`test_dataset = IMDbDataset(test_encodings, test_labels)`**: 最後，創建`test_dataset`來存儲測試集的數據。\n",
    "\n",
    "這樣，通過`IMDbDataset`類別，你現在擁有了三個準備好的數據集，可以用於訓練、驗證和測試DistilBERT等模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段代碼創建了用於訓練、驗證和測試的數據加載器（data loader）。讓我來詳細解釋每一行的意義：\n",
    "\n",
    "1. **`train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)`**:\n",
    "   - `torch.utils.data.DataLoader`是PyTorch中用於加載數據的工具類別，它可以將自定義的`Dataset`對象（這裡是`train_dataset`）轉換為可迭代的數據加載器。\n",
    "   - `train_dataset`是剛剛定義的`IMDbDataset`類別的一個實例，包含了訓練數據的tokenize和編碼數據，以及對應的標籤。\n",
    "   - `batch_size=16`設置了每個批次的樣本數量為16，這意味著每次從數據加載器中獲取的數據批次將包含16個樣本。\n",
    "   - `shuffle=True`表示在每個epoch開始前將訓練數據隨機打亂，這有助於模型更好地學習。\n",
    "\n",
    "2. **`valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)`**:\n",
    "   - 與`train_loader`類似，創建了用於驗證的數據加載器`valid_loader`。\n",
    "   - `valid_dataset`是剛剛創建的`IMDbDataset`類別的另一個實例，包含了驗證數據的tokenize和編碼數據，以及對應的標籤。\n",
    "   - `batch_size=16`與`train_loader`相同，每個批次包含16個樣本。\n",
    "   - `shuffle=False`表示在驗證期間不需要打亂數據，因為驗證過程中不需要計算梯度和反向傳播，因此可以節省計算資源。\n",
    "\n",
    "3. **`test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)`**:\n",
    "   - 與前兩個加載器類似，創建了用於測試的數據加載器`test_loader`。\n",
    "   - `test_dataset`是剛剛創建的`IMDbDataset`類別的另一個實例，包含了測試數據的tokenize和編碼數據，以及對應的標籤。\n",
    "   - `batch_size=16`與前兩個相同，每個批次包含16個樣本。\n",
    "   - `shuffle=False`表示在測試期間不需要打亂數據，以保持數據的原始順序。\n",
    "\n",
    "這樣，通過`train_loader`、`valid_loader`和`test_loader`這三個數據加載器，你可以方便地在訓練、驗證和測試階段使用DistilBERT等模型。每次迭代時，這些加載器將返回一個包含16個樣本的批次，並且在訓練時還會將數據打亂，以幫助模型更好地學習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and fine-tuning a pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段代碼完成了以下操作：\n",
    "\n",
    "1. **`model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')`**：\n",
    "   - 使用`from_pretrained`方法從Hugging Face的模型庫中加載了一個預訓練的DistilBERT模型。這個特定模型是基於小型、輕量化版本的DistilBERT，適合文本分類任務。\n",
    "   - `DistilBertForSequenceClassification`是一個特定於序列分類任務（如情感分析、文本分類等）的DistilBERT模型。它包含了DistilBERT的主體結構以及一個額外的分類層。\n",
    "\n",
    "2. **`model.to(DEVICE)`**：\n",
    "   - 將模型移動到指定的設備上，這裡根據可用的硬件（GPU或CPU）選擇。\n",
    "   - `DEVICE`是在之前定義的，根據系統中是否有可用的GPU，可能是`cuda`（GPU加速）或`cpu`。\n",
    "\n",
    "3. **`model.train()`**：\n",
    "   - 設置模型處於訓練模式。這主要是為了啟用模型中的Dropout層和Batch Normalization層等訓練時特定的行為。\n",
    "   - 在使用`model.train()`後，使用`model.eval()`可以將模型切換回評估模式。\n",
    "\n",
    "4. **`optim = torch.optim.Adam(model.parameters(), lr=5e-5)`**：\n",
    "   - 定義了優化器，這裡使用了Adam優化器（一種常用的隨機梯度下降算法的變種）。\n",
    "   - `model.parameters()`返回模型中所有可學習的參數，這些參數將被優化器用來更新模型的權重。\n",
    "   - `lr=5e-5`設置了學習率，即每次更新參數時的步長大小。這個值通常需要調整來平衡訓練速度和模型性能。\n",
    "\n",
    "總結來說，這段代碼初始化了一個用於序列分類的DistilBERT模型，將其移動到適當的設備（GPU或CPU），設置為訓練模式並創建了一個Adam優化器來最小化訓練過程中的損失。接下來，你可以使用`train_loader`來進行模型的訓練。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段警告信息表明，在初始化`DistilBertForSequenceClassification`時，有一些模型權重並未從`distilbert-base-uncased`模型的檢查點中使用。具體來說：\n",
    "\n",
    "1. **未使用的權重：**\n",
    "   - ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
    "   \n",
    "   這些權重通常與BERT或DistilBERT模型的語言模型頭部（如預訓練或掩碼語言建模任務）相關，而不是用於序列分類任務。\n",
    "\n",
    "2. **新初始化的權重：**\n",
    "   - ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
    "   \n",
    "   這些權重是新初始化的，它們是`DistilBertForSequenceClassification`模型特有的分類頭部的權重。這些權重與預訓練模型中的部分權重不同，因為預訓練模型通常不包括這些分類頭部。\n",
    "\n",
    "3. **警告的含義：**\n",
    "   - 如果你期望從一個與目標任務相同或非常相似的模型檢查點初始化模型，這個警告是正常的。這種情況下，你應該確保模型的預訓練任務和目標任務相符合。\n",
    "   - 如果你期望使用完全相同的模型來初始化，這個警告表明模型檢查點可能不符合預期，你應該確保使用正確的檢查點或進行適當的微調來將模型訓練用於具體的下游任務。\n",
    "\n",
    "總結來說，這個警告提醒你確保模型的初始化與預期任務相符，並建議進行適當的訓練來使模型適應具體的下游任務，以便進行預測和推理操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model -- Manual Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        ### Prepare data\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (predicted_labels == labels).sum()\n",
    "        \n",
    "        return correct_pred.float()/num_examples * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段代碼是用來計算模型在給定數據集上的準確率的函數。讓我們來逐行解釋其含義：\n",
    "\n",
    "```python\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "    with torch.no_grad():  # 確保在評估模式下不進行梯度計算\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "            # 將數據移動到指定的設備（GPU或CPU）\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # 通過模型獲取預測結果\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']  # 獲取模型的預測 logits（未經過 softmax 的輸出）\n",
    "            \n",
    "            # 將 logits 轉換為預測的類別標籤\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            \n",
    "            # 計算準確的預測數量\n",
    "            num_examples += labels.size(0)  # 累計樣本數量\n",
    "            correct_pred += (predicted_labels == labels).sum()  # 累計正確預測的數量\n",
    "        \n",
    "        # 返回準確率（以百分比形式）\n",
    "        return correct_pred.float() / num_examples * 100\n",
    "```\n",
    "\n",
    "這個函數主要用於以下幾個步驟：\n",
    "\n",
    "1. **`with torch.no_grad():`**：\n",
    "   - 這個語句確保在執行函數內部時，PyTorch不會跟蹤和計算梯度，這樣可以節省內存和加速計算。\n",
    "\n",
    "2. **迴圈`for batch_idx, batch in enumerate(data_loader):`**：\n",
    "   - 這個迴圈遍歷了提供的`data_loader`中的每個批次數據。\n",
    "\n",
    "3. **數據準備**：\n",
    "   - `input_ids`、`attention_mask`和`labels`是從每個批次中的字典`batch`中提取的，這些鍵名稱與`IMDbDataset`類中的`__getitem__`方法返回的項目對應。\n",
    "   - 這些數據通過`to(device)`方法移動到指定的設備（GPU或CPU）上進行計算。\n",
    "\n",
    "4. **模型預測**：\n",
    "   - 通過`model(input_ids, attention_mask=attention_mask)`調用模型，將`input_ids`和`attention_mask`傳遞給模型進行前向傳播。\n",
    "   - 從模型輸出中獲取預測的`logits`，這些`logits`是未經過 softmax 處理的輸出結果。\n",
    "\n",
    "5. **計算準確率**：\n",
    "   - 使用`torch.argmax(logits, 1)`獲取每個樣本的預測類別。\n",
    "   - 通過比較`predicted_labels`和`labels`計算正確預測的數量。\n",
    "   - `correct_pred`變量累計正確預測的數量，`num_examples`變量累計處理的總樣本數量。\n",
    "\n",
    "6. **返回結果**：\n",
    "   - 最後計算並返回準確率，即正確預測的百分比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0003 | Batch 0000/2188 | Loss: 0.6771\n",
      "Epoch: 0001/0003 | Batch 0250/2188 | Loss: 0.3006\n",
      "Epoch: 0001/0003 | Batch 0500/2188 | Loss: 0.3678\n",
      "Epoch: 0001/0003 | Batch 0750/2188 | Loss: 0.1487\n",
      "Epoch: 0001/0003 | Batch 1000/2188 | Loss: 0.6674\n",
      "Epoch: 0001/0003 | Batch 1250/2188 | Loss: 0.3264\n",
      "Epoch: 0001/0003 | Batch 1500/2188 | Loss: 0.4358\n",
      "Epoch: 0001/0003 | Batch 1750/2188 | Loss: 0.2579\n",
      "Epoch: 0001/0003 | Batch 2000/2188 | Loss: 0.2474\n",
      "Training accuracy: 96.32%\n",
      "Valid accuracy: 92.34%\n",
      "Time elapsed: 20.67 min\n",
      "Epoch: 0002/0003 | Batch 0000/2188 | Loss: 0.0850\n",
      "Epoch: 0002/0003 | Batch 0250/2188 | Loss: 0.3433\n",
      "Epoch: 0002/0003 | Batch 0500/2188 | Loss: 0.0793\n",
      "Epoch: 0002/0003 | Batch 0750/2188 | Loss: 0.0061\n",
      "Epoch: 0002/0003 | Batch 1000/2188 | Loss: 0.1536\n",
      "Epoch: 0002/0003 | Batch 1250/2188 | Loss: 0.0816\n",
      "Epoch: 0002/0003 | Batch 1500/2188 | Loss: 0.0786\n",
      "Epoch: 0002/0003 | Batch 1750/2188 | Loss: 0.1395\n",
      "Epoch: 0002/0003 | Batch 2000/2188 | Loss: 0.0344\n",
      "Training accuracy: 98.35%\n",
      "Valid accuracy: 92.46%\n",
      "Time elapsed: 41.41 min\n",
      "Epoch: 0003/0003 | Batch 0000/2188 | Loss: 0.0403\n",
      "Epoch: 0003/0003 | Batch 0250/2188 | Loss: 0.0036\n",
      "Epoch: 0003/0003 | Batch 0500/2188 | Loss: 0.0156\n",
      "Epoch: 0003/0003 | Batch 0750/2188 | Loss: 0.0114\n",
      "Epoch: 0003/0003 | Batch 1000/2188 | Loss: 0.1227\n",
      "Epoch: 0003/0003 | Batch 1250/2188 | Loss: 0.0125\n",
      "Epoch: 0003/0003 | Batch 1500/2188 | Loss: 0.0074\n",
      "Epoch: 0003/0003 | Batch 1750/2188 | Loss: 0.0202\n",
      "Epoch: 0003/0003 | Batch 2000/2188 | Loss: 0.0746\n",
      "Training accuracy: 99.08%\n",
      "Valid accuracy: 91.84%\n",
      "Time elapsed: 62.15 min\n",
      "Total Training Time: 62.15 min\n",
      "Test accuracy: 92.50%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        ### Prepare data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "        \n",
    "        ### Backward\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        ### Logging\n",
    "        if not batch_idx % 250:\n",
    "            print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "            \n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'Training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nValid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段代碼是訓練和評估DistilBERT模型的訓練過程，讓我們逐步解釋：\n",
    "\n",
    "1. **開始時間記錄**：\n",
    "   ```python\n",
    "   start_time = time.time()\n",
    "   ```\n",
    "   這行代碼記錄了訓練開始的時間，用於後續計算整個訓練過程的耗時。\n",
    "\n",
    "2. **訓練迴圈**：\n",
    "   ```python\n",
    "   for epoch in range(NUM_EPOCHS):\n",
    "   ```\n",
    "   這是訓練的主迴圈，迭代指定次數的訓練輪次（epochs）。\n",
    "\n",
    "3. **進入訓練模式**：\n",
    "   ```python\n",
    "   model.train()\n",
    "   ```\n",
    "   這行將模型設置為訓練模式，這主要是為了啟用Dropout和Batch Normalization等訓練模式下的特定操作。\n",
    "\n",
    "4. **遍歷數據加載器**：\n",
    "   ```python\n",
    "   for batch_idx, batch in enumerate(train_loader):\n",
    "   ```\n",
    "   在每個訓練輪次中，遍歷訓練數據加載器中的每個批次數據。\n",
    "\n",
    "5. **準備數據**：\n",
    "   ```python\n",
    "   input_ids = batch['input_ids'].to(DEVICE)\n",
    "   attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "   labels = batch['labels'].to(DEVICE)\n",
    "   ```\n",
    "   將從數據加載器中獲取的數據轉移到指定的設備（GPU或CPU）上進行計算。\n",
    "\n",
    "6. **前向傳播**：\n",
    "   ```python\n",
    "   outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "   loss, logits = outputs['loss'], outputs['logits']\n",
    "   ```\n",
    "   通過模型獲取輸出，並從輸出中獲取損失（loss）和預測的 logits。\n",
    "\n",
    "7. **反向傳播和優化**：\n",
    "   ```python\n",
    "   optim.zero_grad()\n",
    "   loss.backward()\n",
    "   optim.step()\n",
    "   ```\n",
    "   這部分代碼執行反向傳播，計算梯度並更新模型的參數。\n",
    "\n",
    "8. **日誌記錄**：\n",
    "   ```python\n",
    "   if not batch_idx % 250:\n",
    "       print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
    "              f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "              f'Loss: {loss:.4f}')\n",
    "   ```\n",
    "   每250個批次打印一次當前的訓練進度（輪次、批次、損失值）。\n",
    "\n",
    "9. **進入評估模式**：\n",
    "   ```python\n",
    "   model.eval()\n",
    "   ```\n",
    "   訓練過程結束後，將模型設置為評估模式，主要是為了在評估過程中禁用Dropout等操作。\n",
    "\n",
    "10. **評估模型**：\n",
    "    ```python\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'Training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nValid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "    ```\n",
    "    使用`compute_accuracy`函數計算並打印訓練集和驗證集的準確率。\n",
    "\n",
    "11. **打印耗時**：\n",
    "    ```python\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    ```\n",
    "    每輪訓練結束後打印耗時時間。\n",
    "\n",
    "12. **總訓練時間**：\n",
    "    ```python\n",
    "    print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "    ```\n",
    "    打印整個訓練過程的總耗時時間。\n",
    "\n",
    "13. **測試集準確率**：\n",
    "    ```python\n",
    "    print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
    "    ```\n",
    "    使用`compute_accuracy`函數計算並打印測試集的準確率。\n",
    "\n",
    "這段代碼完整地實現了訓練過程，包括設置模型、優化器、訓練和評估模式的切換，以及準確率的計算和日誌的打印。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model # free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這行程式碼 `del model` 是在Python中用來刪除物件的語句。在這裡，它的作用是刪除名為 `model` 的變數，這樣可以釋放佔用的記憶體空間，讓系統可以在後續的運算中再次使用這部分記憶體。這對於需要釋放模型佔用的大量記憶體資源時很有用，特別是在處理大型模型或需要長時間執行的程式時。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning a transformer more conveniently using the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(DEVICE)\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段程式碼是用來重新載入並準備一個來自 `distilbert-base-uncased` 預訓練模型的 `DistilBertForSequenceClassification` 實例。讓我們一步步來解釋：\n",
    "\n",
    "1. **`DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')`**：\n",
    "   - 這個函數是從 Hugging Face Transformers 模型庫中載入一個預訓練的 DistilBERT 模型，並初始化為一個序列分類任務的模型。`'distilbert-base-uncased'` 是模型的名稱，表示這是一個小型的基於 DistilBERT 的預訓練模型，支援小寫字母和未標記的分詞。\n",
    "\n",
    "2. **`model.to(DEVICE)`**：\n",
    "   - 將模型移動到指定的計算裝置，這裡的 `DEVICE` 可能是 GPU 或 CPU，取決於系統中可用的硬體資源。通常，如果有 GPU 可用，模型會移動到 GPU 上以加速訓練和推理。\n",
    "\n",
    "3. **`model.train()`**：\n",
    "   - 將模型設置為訓練模式。這告訴模型在訓練過程中要計算梯度，以便後續調用優化器進行參數更新。\n",
    "\n",
    "總結來說，這段程式碼是準備一個可以用於序列分類任務的 DistilBERT 模型，並將其設置為訓練模式，同時移動到適當的計算裝置上。這樣可以開始訓練模型或進行後續的預測和評估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段訊息是來自於 Hugging Face Transformers 套件，告知你在初始化 `DistilBertForSequenceClassification` 模型時發生了一些權重未使用和新初始化的情況。讓我們來解釋一下每個部分的含義：\n",
    "\n",
    "1. **未使用的權重**：\n",
    "   - 這些權重來自於預訓練的 `distilbert-base-uncased` 模型，但在初始化 `DistilBertForSequenceClassification` 時未被使用。具體來說，包括了：\n",
    "     - `vocab_projector.bias`\n",
    "     - `vocab_transform.weight`\n",
    "     - `vocab_projector.weight`\n",
    "     - `vocab_transform.bias`\n",
    "     - `vocab_layer_norm.bias`\n",
    "     - `vocab_layer_norm.weight`\n",
    "   - 這是預期的情況，因為 `DistilBertForSequenceClassification` 可能使用了與預訓練模型不同的架構或在不同的任務上進行了微調。\n",
    "\n",
    "2. **新初始化的權重**：\n",
    "   - 這些權重是在 `DistilBertForSequenceClassification` 初始化過程中新建的，因為它們在預訓練模型中不存在或不直接相關。具體包括：\n",
    "     - `pre_classifier.weight`\n",
    "     - `classifier.weight`\n",
    "     - `classifier.bias`\n",
    "     - `pre_classifier.bias`\n",
    "   - 這些權重是專為序列分類任務而初始化的，通常包括最後一層分類器和與之相關的權重。\n",
    "\n",
    "總結來說，這段訊息提醒你，初始化的模型中有一些權重來自於預訓練模型，但並非所有權重都被重複使用，部分可能是因為模型架構或任務不同所導致的。這是正常的行為，只要確保將模型訓練在你的特定任務上，以便使用它進行預測和推論。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', \n",
    "    num_train_epochs=3,     \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,   \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這段程式碼中，你使用了 Hugging Face Transformers 套件中的 `Trainer` 和 `TrainingArguments` 來進行模型訓練。讓我來解釋一下每行代碼的作用：\n",
    "\n",
    "1. **導入套件**：\n",
    "   ```python\n",
    "   from transformers import Trainer, TrainingArguments\n",
    "   ```\n",
    "   - 導入了 `Trainer` 和 `TrainingArguments` 類別，它們分別用於訓練和訓練參數的設定。\n",
    "\n",
    "2. **定義優化器**：\n",
    "   ```python\n",
    "   optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "   ```\n",
    "   - 定義了一個 Adam 優化器，用於訓練模型。這裡將模型的參數和學習率（lr=5e-5）作為優化器的參數。\n",
    "\n",
    "3. **設置訓練參數**：\n",
    "   ```python\n",
    "   training_args = TrainingArguments(\n",
    "       output_dir='./results', \n",
    "       num_train_epochs=3,     \n",
    "       per_device_train_batch_size=16, \n",
    "       per_device_eval_batch_size=16,   \n",
    "       logging_dir='./logs',\n",
    "       logging_steps=10,\n",
    "   )\n",
    "   ```\n",
    "   - 創建了一個 `TrainingArguments` 物件 `training_args`，用來設置訓練的相關參數。具體參數設置包括：\n",
    "     - `output_dir`：訓練結果和模型儲存的目錄。\n",
    "     - `num_train_epochs`：訓練的總epoch數。\n",
    "     - `per_device_train_batch_size`：每個訓練設備（GPU或CPU）的批次大小。\n",
    "     - `per_device_eval_batch_size`：每個評估設備（GPU或CPU）的批次大小。\n",
    "     - `logging_dir`：日誌文件儲存的目錄。\n",
    "     - `logging_steps`：每隔多少步寫入一次日誌。\n",
    "\n",
    "4. **創建 `Trainer` 實例**：\n",
    "   ```python\n",
    "   trainer = Trainer(\n",
    "       model=model,\n",
    "       args=training_args,\n",
    "       train_dataset=train_dataset,\n",
    "   )\n",
    "   ```\n",
    "   - 使用創建的 `model` 和 `training_args` 來實例化 `Trainer` 物件 `trainer`。`Trainer` 將管理模型訓練的所有過程，包括前向傳播、反向傳播、優化器更新等。\n",
    "\n",
    "這段程式碼設置了一個基本的訓練流程，使用了 `Trainer` 和 `TrainingArguments` 來方便地進行模型訓練，同時管理訓練參數和日誌記錄。接下來，你可以使用 `trainer.train()` 方法來開始訓練模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dataset via pip install datasets\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred # logits are a numpy array, not pytorch tensor\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(\n",
    "               predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段程式碼主要用於設定並使用 `datasets` 函式庫來加載評估指標（accuracy），並定義了一個計算評估指標的函式 `compute_metrics`。讓我們逐行來解釋每段程式碼的意義：\n",
    "\n",
    "1. **安裝 `datasets` 函式庫**：\n",
    "   ```python\n",
    "   # install dataset via pip install datasets\n",
    "   ```\n",
    "   - 這是一個註解，指示你可以通過使用 `pip install datasets` 來安裝 `datasets` 函式庫，它提供了訪問各種常用於機器學習的數據集和指標的介面。\n",
    "\n",
    "2. **導入必要的庫**：\n",
    "   ```python\n",
    "   from datasets import load_metric\n",
    "   import numpy as np\n",
    "   ```\n",
    "   - 導入 `datasets` 函式庫中的 `load_metric` 函式，用於加載特定的指標（在這裡是 `accuracy`）。\n",
    "   - 導入 `numpy` 庫並縮寫為 `np`，這將用於數組操作。\n",
    "\n",
    "3. **加載指標**：\n",
    "   ```python\n",
    "   metric = load_metric(\"accuracy\")\n",
    "   ```\n",
    "   - 使用 `load_metric` 函式從 `datasets` 函式庫中加載 `accuracy` 指標。這個指標將用於在模型評估期間計算評估指標。\n",
    "\n",
    "4. **定義 `compute_metrics` 函式**：\n",
    "   ```python\n",
    "   def compute_metrics(eval_pred):\n",
    "       logits, labels = eval_pred  # logits 是一個 numpy 數組，不是 pytorch 張量\n",
    "       predictions = np.argmax(logits, axis=-1)\n",
    "       return metric.compute(predictions=predictions, references=labels)\n",
    "   ```\n",
    "   - `compute_metrics` 函式接受一個 `eval_pred` 參數，其中包含模型預測的 logits（對應於類別的分數）和真實的標籤 `labels`。\n",
    "   - 使用 `numpy` 的 `argmax` 函式來計算每個樣本預測的類別索引，`axis=-1` 表示在最後一個維度上執行求取最大值的操作。\n",
    "   - 最後，調用 `metric.compute` 函式計算並返回預測結果 `predictions` 與真實標籤 `labels` 之間的評估指標（在這裡是準確度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', \n",
    "    num_train_epochs=3,     \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,   \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(optim, None) # optimizer and learning rate scheduler\n",
    ")\n",
    "\n",
    "# force model to only use 1 GPU (even if multiple are availabe)\n",
    "# to compare more fairly to previous code\n",
    "\n",
    "trainer.args._n_gpu = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段程式碼是在設置與使用 `Trainer` 物件來進行模型的訓練和評估。讓我們來逐行解釋每段程式碼的意義：\n",
    "\n",
    "1. **設置優化器**：\n",
    "   ```python\n",
    "   optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "   ```\n",
    "   - 創建了一個 Adam 優化器來優化模型參數，學習率設置為 `5e-5`。\n",
    "\n",
    "2. **設置訓練參數**：\n",
    "   ```python\n",
    "   training_args = TrainingArguments(\n",
    "       output_dir='./results', \n",
    "       num_train_epochs=3,     \n",
    "       per_device_train_batch_size=16, \n",
    "       per_device_eval_batch_size=16,   \n",
    "       logging_dir='./logs',\n",
    "       logging_steps=10\n",
    "   )\n",
    "   ```\n",
    "   - 創建了一個 `TrainingArguments` 對象，這裡設置了模型訓練的各種參數，包括：\n",
    "     - `output_dir`: 模型訓練過程中輸出的目錄。\n",
    "     - `num_train_epochs`: 訓練的總 epoch 數。\n",
    "     - `per_device_train_batch_size`: 每個設備上的訓練批次大小。\n",
    "     - `per_device_eval_batch_size`: 每個設備上的評估批次大小。\n",
    "     - `logging_dir`: 訓練過程中日誌輸出的目錄。\n",
    "     - `logging_steps`: 每隔多少步驟記錄一次訓練日誌。\n",
    "\n",
    "3. **創建 `Trainer` 物件**：\n",
    "   ```python\n",
    "   trainer = Trainer(\n",
    "       model=model,\n",
    "       compute_metrics=compute_metrics,\n",
    "       args=training_args,\n",
    "       train_dataset=train_dataset,\n",
    "       eval_dataset=test_dataset,\n",
    "       optimizers=(optim, None) # optimizer and learning rate scheduler\n",
    "   )\n",
    "   ```\n",
    "   - 創建了一個 `Trainer` 物件，用於訓練和評估模型。\n",
    "   - `model`: 設置要訓練的模型。\n",
    "   - `compute_metrics`: 設置用於計算評估指標的函式。\n",
    "   - `args`: 設置訓練的參數，這裡使用了之前定義的 `training_args`。\n",
    "   - `train_dataset`: 設置訓練所使用的數據集。\n",
    "   - `eval_dataset`: 設置評估所使用的數據集。\n",
    "   - `optimizers`: 設置優化器和學習率調度器，這裡只設置了優化器為之前定義的 `optim`。\n",
    "\n",
    "4. **強制使用單 GPU**：\n",
    "   ```python\n",
    "   trainer.args._n_gpu = 1\n",
    "   ```\n",
    "   - 強制設置 `Trainer` 物件只使用一個 GPU 進行訓練，即使有多個 GPU 可用，這樣可以更公平地與之前的程式碼進行比較。\n",
    "\n",
    "這樣的設置使得可以通過 `Trainer` 物件更方便地管理模型訓練的過程，包括了記錄日誌、處理不同的設備、管理優化器等功能，使得整個訓練過程更加方便和高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個警告訊息是關於未來版本中 PyTorch 相關訓練參數 `--report_to` 默認值的更改。目前的警告建議在未來的 v5 版本中，默認值將從當前的所有安裝的集成更改為 `none`。這意味著在未來版本中，如果需要保留相同的行為，就需要明確地設置 `--report_to all`。\n",
    "\n",
    "根據警告信息，建議開始更新程式碼，並確保在將來的版本中能夠正確處理這些變更，以避免潛在的影響或錯誤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 35000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6564\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6564' max='6564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6564/6564 45:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.591600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.328600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.426300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.356900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.470800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.405700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.294300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.310300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.313700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.367500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.270900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.274900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.300300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.249200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.262100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.305200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.268100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.255700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.324200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.246900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.335300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.321400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.429300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.344300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.299100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.270900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.291900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.288600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.238200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.266900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.229300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.235800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.260600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.222100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.420300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.364400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.315300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.220900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.154200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.257600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.175100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.230600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.179400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.286100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.272100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>0.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.240300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.176800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>0.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>0.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>0.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>0.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>0.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>0.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>0.254700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>0.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.123300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>0.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>0.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>0.191700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.093700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>0.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>0.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>0.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>0.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.197900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.131700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>0.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>0.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.127300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>0.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>0.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>0.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>0.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2930</td>\n",
       "      <td>0.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>0.094100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>0.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>0.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>0.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>0.095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>0.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>0.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>0.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>0.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.144200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>0.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3170</td>\n",
       "      <td>0.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3190</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3210</td>\n",
       "      <td>0.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>0.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3230</td>\n",
       "      <td>0.135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.169700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>0.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3270</td>\n",
       "      <td>0.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3310</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>0.102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>0.112700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>0.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3370</td>\n",
       "      <td>0.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>0.093300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3390</td>\n",
       "      <td>0.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3410</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>0.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3430</td>\n",
       "      <td>0.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>0.167300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3470</td>\n",
       "      <td>0.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3490</td>\n",
       "      <td>0.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3510</td>\n",
       "      <td>0.086900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>0.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>0.174700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>0.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3570</td>\n",
       "      <td>0.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>0.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3590</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3610</td>\n",
       "      <td>0.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3630</td>\n",
       "      <td>0.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>0.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3670</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>0.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3690</td>\n",
       "      <td>0.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3710</td>\n",
       "      <td>0.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>0.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3730</td>\n",
       "      <td>0.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>0.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3770</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3790</td>\n",
       "      <td>0.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3810</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>0.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3830</td>\n",
       "      <td>0.175100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>0.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>0.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3870</td>\n",
       "      <td>0.100900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>0.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3890</td>\n",
       "      <td>0.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>0.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>0.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3930</td>\n",
       "      <td>0.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>0.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>0.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3970</td>\n",
       "      <td>0.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>0.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3990</td>\n",
       "      <td>0.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4010</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>0.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4030</td>\n",
       "      <td>0.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>0.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.031500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>0.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4070</td>\n",
       "      <td>0.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4090</td>\n",
       "      <td>0.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.098300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4110</td>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>0.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4130</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170</td>\n",
       "      <td>0.250300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>0.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4190</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4210</td>\n",
       "      <td>0.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>0.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4230</td>\n",
       "      <td>0.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>0.081900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.176500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4270</td>\n",
       "      <td>0.186100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>0.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4290</td>\n",
       "      <td>0.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4310</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4330</td>\n",
       "      <td>0.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>0.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>0.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4370</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>0.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4390</td>\n",
       "      <td>0.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4410</td>\n",
       "      <td>0.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4430</td>\n",
       "      <td>0.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4470</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4490</td>\n",
       "      <td>0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4510</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>0.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4530</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>0.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4570</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4590</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4610</td>\n",
       "      <td>0.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>0.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4630</td>\n",
       "      <td>0.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4640</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4660</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4670</td>\n",
       "      <td>0.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>0.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4690</td>\n",
       "      <td>0.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4710</td>\n",
       "      <td>0.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4720</td>\n",
       "      <td>0.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4730</td>\n",
       "      <td>0.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4740</td>\n",
       "      <td>0.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4770</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4780</td>\n",
       "      <td>0.099700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4790</td>\n",
       "      <td>0.063600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4810</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4820</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4830</td>\n",
       "      <td>0.086400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4840</td>\n",
       "      <td>0.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4860</td>\n",
       "      <td>0.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4870</td>\n",
       "      <td>0.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4880</td>\n",
       "      <td>0.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4890</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4910</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4930</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4940</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4960</td>\n",
       "      <td>0.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4970</td>\n",
       "      <td>0.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>0.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4990</td>\n",
       "      <td>0.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5010</td>\n",
       "      <td>0.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5020</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5030</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5040</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5060</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5070</td>\n",
       "      <td>0.103100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5080</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5090</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5110</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>0.110200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5130</td>\n",
       "      <td>0.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5140</td>\n",
       "      <td>0.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5160</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5170</td>\n",
       "      <td>0.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5180</td>\n",
       "      <td>0.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5190</td>\n",
       "      <td>0.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5210</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5220</td>\n",
       "      <td>0.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5230</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5240</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5260</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5270</td>\n",
       "      <td>0.083500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5280</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5290</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5310</td>\n",
       "      <td>0.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5320</td>\n",
       "      <td>0.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5330</td>\n",
       "      <td>0.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5340</td>\n",
       "      <td>0.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5360</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5370</td>\n",
       "      <td>0.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5380</td>\n",
       "      <td>0.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5390</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5410</td>\n",
       "      <td>0.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5420</td>\n",
       "      <td>0.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5430</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5440</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5460</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5470</td>\n",
       "      <td>0.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5480</td>\n",
       "      <td>0.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5490</td>\n",
       "      <td>0.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5510</td>\n",
       "      <td>0.145400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5520</td>\n",
       "      <td>0.039300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5530</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5540</td>\n",
       "      <td>0.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5560</td>\n",
       "      <td>0.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5580</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5590</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5610</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5620</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5630</td>\n",
       "      <td>0.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5640</td>\n",
       "      <td>0.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5660</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5670</td>\n",
       "      <td>0.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5680</td>\n",
       "      <td>0.103500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5690</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5710</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5720</td>\n",
       "      <td>0.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5730</td>\n",
       "      <td>0.081200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5740</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5760</td>\n",
       "      <td>0.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5770</td>\n",
       "      <td>0.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5780</td>\n",
       "      <td>0.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5790</td>\n",
       "      <td>0.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5810</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5820</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5840</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5860</td>\n",
       "      <td>0.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5870</td>\n",
       "      <td>0.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5880</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5890</td>\n",
       "      <td>0.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.072500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5910</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5920</td>\n",
       "      <td>0.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5930</td>\n",
       "      <td>0.170300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5940</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5960</td>\n",
       "      <td>0.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5970</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5980</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5990</td>\n",
       "      <td>0.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6010</td>\n",
       "      <td>0.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6020</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6030</td>\n",
       "      <td>0.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6040</td>\n",
       "      <td>0.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6060</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6070</td>\n",
       "      <td>0.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6080</td>\n",
       "      <td>0.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6090</td>\n",
       "      <td>0.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6110</td>\n",
       "      <td>0.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6120</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6130</td>\n",
       "      <td>0.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6140</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6160</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6170</td>\n",
       "      <td>0.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6180</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6190</td>\n",
       "      <td>0.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6210</td>\n",
       "      <td>0.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6220</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6230</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6240</td>\n",
       "      <td>0.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6260</td>\n",
       "      <td>0.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6270</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6280</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6290</td>\n",
       "      <td>0.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6310</td>\n",
       "      <td>0.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6320</td>\n",
       "      <td>0.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6330</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6340</td>\n",
       "      <td>0.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6360</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6370</td>\n",
       "      <td>0.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6380</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6390</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6410</td>\n",
       "      <td>0.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6420</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6430</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6440</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6460</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6470</td>\n",
       "      <td>0.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6480</td>\n",
       "      <td>0.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6490</td>\n",
       "      <td>0.077200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6510</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6520</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6530</td>\n",
       "      <td>0.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6540</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6560</td>\n",
       "      <td>0.020400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n",
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-3500\n",
      "Configuration saved in ./results/checkpoint-3500/config.json\n",
      "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n",
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-4500\n",
      "Configuration saved in ./results/checkpoint-4500/config.json\n",
      "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-5000\n",
      "Configuration saved in ./results/checkpoint-5000/config.json\n",
      "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-5500\n",
      "Configuration saved in ./results/checkpoint-5500/config.json\n",
      "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-6000\n",
      "Configuration saved in ./results/checkpoint-6000/config.json\n",
      "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-6500\n",
      "Configuration saved in ./results/checkpoint-6500/config.json\n",
      "Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time: 45.36 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainer.train()\n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段程式碼是使用 `Trainer` 物件來進行模型訓練的準備與設置。讓我們來詳細解釋每一行的意義：\n",
    "\n",
    "1. **start_time = time.time()**: 記錄當前時間，用於後續計算訓練所花費的時間。\n",
    "\n",
    "2. **trainer.train()**: 啟動訓練過程。這裡的 `trainer` 是之前設置好的 `Trainer` 物件，它包含了模型、訓練參數以及訓練數據集等設定。\n",
    "\n",
    "3. **print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')**: 計算並輸出整個訓練過程所花費的時間，以分鐘為單位。`time.time() - start_time` 計算從 `start_time` 到當前的時間差，除以 60 得到分鐘數，`.2f` 表示保留兩位小數。\n",
    "\n",
    "這段程式碼的目的是訓練 `Trainer` 物件中指定的模型，並監控訓練時間。通過 `Trainer` 對象，可以更容易地管理和設置訓練過程中的各種參數和資源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段訓練過程的日誌信息顯示了訓練的基本設置和參數，讓我們逐一解釋：\n",
    "\n",
    "- **Num examples = 35000**: 訓練數據集中的樣本數量，這裡是35000個樣本。\n",
    "\n",
    "- **Num Epochs = 3**: 訓練過程中的 epoch 數量，即完整遍歷訓練數據集的次數。\n",
    "\n",
    "- **Instantaneous batch size per device = 16**: 每個訓練設備（如 GPU）的即時批次大小，即每次訓練時，每個設備處理的樣本數量。\n",
    "\n",
    "- **Total train batch size (w. parallel, distributed & accumulation) = 16**: 總的訓練批次大小，考慮到平行處理、分佈式訓練和梯度累積等因素後的批次大小。\n",
    "\n",
    "- **Gradient Accumulation steps = 1**: 梯度累積步驟，即在執行反向傳播之前累積梯度的步數。這裡設置為1，表示每個批次後立即執行反向傳播更新模型參數。\n",
    "\n",
    "- **Total optimization steps = 6564**: 總的優化步驟數，即總共需要更新模型參數的次數。計算方式是 `Num examples * Num Epochs / Total train batch size`，這裡的值是根據上述參數計算出來的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.30534815788269043,\n",
       " 'eval_accuracy': 0.9327,\n",
       " 'eval_runtime': 87.1161,\n",
       " 'eval_samples_per_second': 114.789,\n",
       " 'eval_steps_per_second': 7.174,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trainer.evaluate()` 的功能是用來評估模型在驗證集或測試集上的性能表現。根據之前的設置，`eval_dataset` 參數應該是設置為測試集 `test_dataset`，因此 `trainer.evaluate()` 會計算模型在測試集上的指標，如準確率、損失等，具體取決於在 `compute_metrics` 函數中定義的評估指標。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起來你正在觀察訓練過程中的進度更新。這個更新顯示了模型訓練過程中的批次進度，其中 `625/625` 表示目前訓練到的批次數量，`01:26` 則表示已經過去的時間（小時:分鐘）。這些數據幫助你了解訓練的進度和時間消耗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這些指標是在模型評估（evaluate）過程中計算得到的結果。讓我們一一解釋這些指標的含義：\n",
    "\n",
    "- `eval_loss`: 在評估過程中計算得到的平均損失值。這表示模型在測試集上的預測與實際標籤之間的差異程度。\n",
    "  \n",
    "- `eval_accuracy`: 在評估過程中計算得到的準確率。這表示模型在測試集上正確預測的比例，通常以百分比表示。\n",
    "\n",
    "- `eval_runtime`: 評估過程的執行時間，單位是秒。\n",
    "\n",
    "- `eval_samples_per_second`: 評估過程中每秒處理的樣本數。\n",
    "\n",
    "- `eval_steps_per_second`: 評估過程中每秒完成的步驟數。\n",
    "\n",
    "- `epoch`: 目前的訓練周期數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 93.27%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這段程式碼中，我們將模型設置為評估模式（eval mode），然後將其移動到適當的裝置（DEVICE）。接著，我們使用之前定義的 `compute_accuracy` 函式來計算模型在測試集上的準確率，並將結果以百分比形式印出。\n",
    "\n",
    "這段程式碼的流程如下：\n",
    "1. `model.eval()`: 將模型設置為評估模式。在這個模式下，模型的行為與訓練模式不同，特別是在評估過程中不會計算梯度或者進行反向傳播。\n",
    "   \n",
    "2. `model.to(DEVICE)`: 將模型移動到指定的裝置（這裡是 CUDA 裝置，如果可用的話）。\n",
    "\n",
    "3. `compute_accuracy(model, test_loader, DEVICE)`: 調用 `compute_accuracy` 函式計算模型在測試集上的準確率。\n",
    "\n",
    "4. `print(f'Test accuracy: {accuracy:.2f}%')`: 將計算得到的測試準確率印出，並格式化成百分比形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試準確率為 93.27%。這表示在測試集上，模型對情感分類任務的準確率為93.27%。這是一個很好的結果，顯示了模型在這個任務上的良好表現。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Readers may ignore the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n",
      "[NbConvertApp] Converting notebook ch16-part3-bert.ipynb to script\n",
      "[NbConvertApp] Writing 9089 bytes to ch16-part3-bert.py\n"
     ]
    }
   ],
   "source": [
    "! python ../.convert_notebook_to_script.py --input ch16-part3-bert.ipynb --output ch16-part3-bert.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_lstm_packed_imdb.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
